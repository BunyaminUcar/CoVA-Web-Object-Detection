{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from datasets import load_data\n",
    "from models import WebObjExtractionNet\n",
    "from train import train_model, evaluate_model\n",
    "from utils import print_and_log\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N_CLASSES = 4\n",
    "CLASS_NAMES = ['BG', 'Price', 'Image', 'Title']\n",
    "IMG_HEIGHT = 1280 # Image assumed to have same height and width\n",
    "EVAL_INTERVAL = 5 # Number of Epochs after which model is evaluated\n",
    "DATA_DIR = '../data/' # Contains .png and .pkl files for train and test data\n",
    "OUTPUT_DIR = 'results' # logs are saved here!\n",
    "MODEL_SAVE_DIR = 'saved_models' # trained models are saved here!\n",
    "TRAIN_SPLIT_ID_FILE = 'train_imgs.txt' # each line should contain name of the training image (without file extension)\n",
    "TEST_SPLIT_ID_FILE = 'test_imgs.txt'\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "if not os.path.exists(MODEL_SAVE_DIR):\n",
    "    os.makedirs(MODEL_SAVE_DIR)\n",
    "\n",
    "train_img_ids = np.loadtxt(TRAIN_SPLIT_ID_FILE, dtype=np.int32)\n",
    "test_img_ids = np.loadtxt(TEST_SPLIT_ID_FILE, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3\n",
    "BACKBONE = 'alexnet'\n",
    "BATCH_SIZE = 50\n",
    "ROI_POOL_OUTPUT_SIZE = (3,3)\n",
    "TRAINABLE_CONVNET = True\n",
    "WEIGHTED_LOSS = False\n",
    "\n",
    "if WEIGHTED_LOSS:\n",
    "    weights = torch.Tensor([1,100,100,100]) # weight inversely proportional to number of examples for the class\n",
    "    print('Weighted loss with class weights:', weights)\n",
    "else:\n",
    "    weights = torch.ones(N_CLASSES)\n",
    "\n",
    "# load train/test data loaders\n",
    "train_loader, test_loader = load_data(DATA_DIR, train_img_ids, test_img_ids, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: if same hyperparameter configuration is run again, previous log file will be overwritten\n",
    "params = '%s batch-%d roi-%d lr-%.0e wt_loss-%d' % (BACKBONE, BATCH_SIZE, ROI_POOL_OUTPUT_SIZE[0], LEARNING_RATE, WEIGHTED_LOSS)\n",
    "log_file = '%s/logs %s.txt' % (OUTPUT_DIR, params)\n",
    "model_save_file = '%s/saved_model %s.pth' % (MODEL_SAVE_DIR, params)\n",
    "\n",
    "print('logs will be saved in \\\"%s\\\"' % (log_file))\n",
    "print_and_log('Batch Size: %d' % (BATCH_SIZE), log_file, 'w')\n",
    "print_and_log('RoI Pool Output Size: (%d, %d)' % ROI_POOL_OUTPUT_SIZE, log_file)\n",
    "print_and_log('Learning Rate: %.0e\\n' % (LEARNING_RATE), log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = WebObjExtractionNet(ROI_POOL_OUTPUT_SIZE, IMG_HEIGHT, N_CLASSES, BACKBONE, TRAINABLE_CONVNET, CLASS_NAMES).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights, reduction='sum').to(device)\n",
    "\n",
    "model = train_model(model, train_loader, optimizer, criterion, N_EPOCHS, device, test_loader, EVAL_INTERVAL, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_save_file)\n",
    "print_and_log('Model can be restored from \\\"%s\\\"' % (model_save_file), log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
